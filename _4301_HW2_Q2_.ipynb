{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "-- 4301 -- HW2 -- Q2 --.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkEEg3dY9mQJxOC5YJjwQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahabshms/Numerical_Methods_for_ML_and_AI_Solution_2/blob/main/_4301_HW2_Q2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVYTMBHsQZwH"
      },
      "source": [
        "#**Problem 2: Projections onto Convex Hulls** \n",
        "For this problem, you will implement the Frank-Wolfe algorithm using scipy.linprog in the Python scipy package to help you solve the per-iteration subproblems. Recall that, given $x^{(1)}, \\dots , x^{(M)} \\in \\mathbb{R}^n$\n",
        ", their convex hull is the set of all $x$ that can be written as a convex combination of these points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU9-mc0YQ6DD"
      },
      "source": [
        "1. Use the Frank-Wolfe algorithm to compute the projection of a query point $q$ onto the convex hull of the given $x^{(1)},\\dots , x^{(M)} \\in \\mathbb{R}^n$. That is, you should solve the following optimization problem.\n",
        "\\begin{align}\n",
        "\\min_{\\lambda \\in \\mathbb{R}^n} \\frac{1}{2}\\Vert q - \\sum_{m=1}^M \\lambda_m x^{(m)} \\Vert_2^2\n",
        "\\end{align}\n",
        "such that\n",
        "\\begin{align}\n",
        "\\sum_{m=1}^M \\lambda_m &= 1\\\\\n",
        "\\lambda &\\succeq 0\n",
        "\\end{align}\n",
        "Your Python function should take as input the $x$â€™s, the query point $q$, an initial value for $\\lambda$ that satisfies the constraints, and the tolerance for the Frank-Wolfe convergence condition and return the best function value found during the iterative procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8xzAxsOoFS9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy8W78-GSgnR"
      },
      "source": [
        "**Conditional Gradients (Franke-Wolfe)**\n",
        "\n",
        "Formally\n",
        "\\begin{align}\n",
        "f_0(x) \\approx f_0(x_t) + \\nabla f_0(x_t)^T(x-x_t).\n",
        "\\end{align}\n",
        "Compute\n",
        "\\begin{align}\n",
        "s_t \\in \\text{argmin}_{x\\in C} \\nabla f_0(x_t)^T x\n",
        "\\end{align}\n",
        "Set\n",
        "\\begin{align}\n",
        "x_{t+1} = x_t + \\gamma_t(s_t - x_t)\n",
        "\\end{align}\n",
        "The algorithm is guaranteed to converge with a diminishing step size choice like\n",
        "\\begin{align}\n",
        "\\gamma_{t} = \\frac{2}{2+t}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2IcvuedQMtv"
      },
      "source": [
        "So\n",
        "\\begin{align}\n",
        "f(\\lambda) = &\\frac{1}{2}\\Vert q - \\sum_{m=1}^M \\lambda_m x^{(m)} \\Vert_2^2\\\\\n",
        "&= \\frac{1}{2} \\sum_{i=1}^{n} \\left(q_i - \\sum_{m=1}^M \\lambda_mx^{(m)}_i\\right)^2\\\\\n",
        "\\nabla f(\\lambda)_j &=  - \\sum_{i = 1}^n x_i^{(j)}\\left(q_i - \\sum_{m=1}^M \\lambda_m x_i^{(m)}\\right)  \n",
        "\\end{align} \\\\\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lh8iqDSbiEV"
      },
      "source": [
        "def get_gradient(lambd_a):\n",
        "  gradient = np.zeros([M])\n",
        "  for j in range(gradient.shape[0]):\n",
        "    sum_n = 0\n",
        "    for i in range(n):\n",
        "      sum_m = 0\n",
        "      for m in range(gradient.shape[0]):\n",
        "\n",
        "        sum_m += lambd_a[m]*x[m][i]\n",
        "\n",
        "      sum_n += x[j][i] * (q[i] - sum_m)\n",
        "\n",
        "    gradient[j] = -sum_n\n",
        "\n",
        "  return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpoCXVQSYhhm"
      },
      "source": [
        "To solve the per-iteration subproblems, use scipy\n",
        "\\begin{align}\n",
        "s_t &= \\text{argmin}_{\\substack{\\lambda \\in [0,1]^M\\\\\\sum_{i=1}^M \\lambda_i = 1}}\\nabla f(\\lambda^{\\text{old}})^T \\lambda\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Lo546lQExe"
      },
      "source": [
        "def s_finder(lambd_a):\n",
        "  \n",
        "  gradient = get_gradient(lambd_a)\n",
        "  M = gradient.shape[0]\n",
        "  l = 0\n",
        "  u = 1\n",
        "  A_eq = np.ones([1,M])\n",
        "  b_eq = np.ones([1,1])\n",
        "  s_t = optimize.linprog(c = gradient, bounds = [l,u], A_eq = A_eq,b_eq = b_eq)\n",
        "  \n",
        "  return s_t.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-kfTszvYc2K"
      },
      "source": [
        "And then\n",
        "\\begin{align}\\lambda^{\\text{new}} &= \\lambda^{\\text{old}} + \\gamma (s_t - \\lambda^{\\text{old}})\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKaw84wHYixA"
      },
      "source": [
        "def Frank_Wolfe(max_iterations,initial_lambd_a,epsilon):\n",
        "  lambd_a = initial_lambd_a\n",
        "\n",
        "  for iteration in range(1,max_iterations):\n",
        "\n",
        "    gamma = 2 / (2 + iteration)\n",
        "\n",
        "    s_t = s_finder(lambd_a)\n",
        "    step = gamma*(s_t - lambd_a)\n",
        "\n",
        "    if sum(abs(step)) < epsilon: #Stopping Criteria\n",
        "      break\n",
        "\n",
        "    lambd_a = lambd_a + step\n",
        "\n",
        "\n",
        "  return lambd_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OJ8tUN5Z8n0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5ede12-975b-44ec-c42a-059ca3184bd1"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "\n",
        "x = np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
        "q = np.array([1.2,0.2])\n",
        "\n",
        "n = 2\n",
        "M = 4\n",
        "\n",
        "max_iterations = 1000\n",
        "initial_lambd_a = np.zeros(x.shape[0])\n",
        "epsilon = 0.00001\n",
        "\n",
        "\n",
        "final_lambda = Frank_Wolfe(max_iterations,initial_lambd_a,epsilon)\n",
        "print(final_lambda)\n",
        "point = np.zeros(n)\n",
        "for m in range(M):\n",
        "  point += final_lambda[m]*x[m]\n",
        "print(point)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.76927988e-10 2.76408737e-10 4.00799284e-01 5.99198718e-01]\n",
            "[0.999998   0.19839943]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}